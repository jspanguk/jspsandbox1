{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is :  tensor([1.], requires_grad=True)\n",
      "b is :  tensor([0.], requires_grad=True)\n",
      "W.grad is :  None\n",
      "b.grad is :  None\n",
      "cost is :  tensor(6.5000, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  None\n",
      "b.grad is zero:  None\n",
      "W.grad is backward:  tensor([-8.])\n",
      "b.grad is backward:  tensor([-5.])\n",
      "Epoch 0/10 W: 1.800, W.grad: -8.0 b: 0.5 Cost: 6.500000\n",
      "\n",
      "w is :  tensor([1.8000], requires_grad=True)\n",
      "b is :  tensor([0.5000], requires_grad=True)\n",
      "W.grad is :  tensor([-8.])\n",
      "b.grad is :  tensor([-5.])\n",
      "cost is :  tensor(0.6500, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([-2.5000])\n",
      "b.grad is backward:  tensor([-1.6000])\n",
      "Epoch 1/10 W: 2.050, W.grad: -2.500000238418579 b: 0.6600000262260437 Cost: 0.650000\n",
      "\n",
      "w is :  tensor([2.0500], requires_grad=True)\n",
      "b is :  tensor([0.6600], requires_grad=True)\n",
      "W.grad is :  tensor([-2.5000])\n",
      "b.grad is :  tensor([-1.6000])\n",
      "cost is :  tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([-0.7700])\n",
      "b.grad is backward:  tensor([-0.5300])\n",
      "Epoch 2/10 W: 2.127, W.grad: -0.7700004577636719 b: 0.7130000591278076 Cost: 0.070850\n",
      "\n",
      "w is :  tensor([2.1270], requires_grad=True)\n",
      "b is :  tensor([0.7130], requires_grad=True)\n",
      "W.grad is :  tensor([-0.7700])\n",
      "b.grad is :  tensor([-0.5300])\n",
      "cost is :  tensor(0.0133, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([-0.2260])\n",
      "b.grad is backward:  tensor([-0.1930])\n",
      "Epoch 3/10 W: 2.150, W.grad: -0.2259998321533203 b: 0.7323000431060791 Cost: 0.013344\n",
      "\n",
      "w is :  tensor([2.1496], requires_grad=True)\n",
      "b is :  tensor([0.7323], requires_grad=True)\n",
      "W.grad is :  tensor([-0.2260])\n",
      "b.grad is :  tensor([-0.1930])\n",
      "cost is :  tensor(0.0075, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([-0.0551])\n",
      "b.grad is backward:  tensor([-0.0866])\n",
      "Epoch 4/10 W: 2.155, W.grad: -0.055100202560424805 b: 0.7409600615501404 Cost: 0.007470\n",
      "\n",
      "w is :  tensor([2.1551], requires_grad=True)\n",
      "b is :  tensor([0.7410], requires_grad=True)\n",
      "W.grad is :  tensor([-0.0551])\n",
      "b.grad is :  tensor([-0.0866])\n",
      "cost is :  tensor(0.0067, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([-0.0016])\n",
      "b.grad is backward:  tensor([-0.0527])\n",
      "Epoch 5/10 W: 2.155, W.grad: -0.0015690326690673828 b: 0.7462350130081177 Cost: 0.006710\n",
      "\n",
      "w is :  tensor([2.1553], requires_grad=True)\n",
      "b is :  tensor([0.7462], requires_grad=True)\n",
      "W.grad is :  tensor([-0.0016])\n",
      "b.grad is :  tensor([-0.0527])\n",
      "cost is :  tensor(0.0065, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([0.0150])\n",
      "b.grad is backward:  tensor([-0.0417])\n",
      "Epoch 6/10 W: 2.154, W.grad: 0.015039920806884766 b: 0.7504079341888428 Cost: 0.006462\n",
      "\n",
      "w is :  tensor([2.1538], requires_grad=True)\n",
      "b is :  tensor([0.7504], requires_grad=True)\n",
      "W.grad is :  tensor([0.0150])\n",
      "b.grad is :  tensor([-0.0417])\n",
      "cost is :  tensor(0.0063, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([0.0200])\n",
      "b.grad is backward:  tensor([-0.0379])\n",
      "Epoch 7/10 W: 2.152, W.grad: 0.020038604736328125 b: 0.7541974782943726 Cost: 0.006270\n",
      "\n",
      "w is :  tensor([2.1518], requires_grad=True)\n",
      "b is :  tensor([0.7542], requires_grad=True)\n",
      "W.grad is :  tensor([0.0200])\n",
      "b.grad is :  tensor([-0.0379])\n",
      "cost is :  tensor(0.0061, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([0.0214])\n",
      "b.grad is backward:  tensor([-0.0363])\n",
      "Epoch 8/10 W: 2.150, W.grad: 0.021388530731201172 b: 0.7578302025794983 Cost: 0.006088\n",
      "\n",
      "w is :  tensor([2.1496], requires_grad=True)\n",
      "b is :  tensor([0.7578], requires_grad=True)\n",
      "W.grad is :  tensor([0.0214])\n",
      "b.grad is :  tensor([-0.0363])\n",
      "cost is :  tensor(0.0059, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([0.0216])\n",
      "b.grad is backward:  tensor([-0.0355])\n",
      "Epoch 9/10 W: 2.147, W.grad: 0.021591901779174805 b: 0.7613781094551086 Cost: 0.005911\n",
      "\n",
      "w is :  tensor([2.1475], requires_grad=True)\n",
      "b is :  tensor([0.7614], requires_grad=True)\n",
      "W.grad is :  tensor([0.0216])\n",
      "b.grad is :  tensor([-0.0355])\n",
      "cost is :  tensor(0.0057, grad_fn=<MeanBackward0>)\n",
      "W.grad is zero:  tensor([0.])\n",
      "b.grad is zero:  tensor([0.])\n",
      "W.grad is backward:  tensor([0.0214])\n",
      "b.grad is backward:  tensor([-0.0349])\n",
      "Epoch 10/10 W: 2.145, W.grad: 0.021440505981445312 b: 0.7648641467094421 Cost: 0.005740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 x를 입력했을때 순전파를 통해 y의 값을 내놓는 선형회귀\n",
    "x_train = torch.FloatTensor([[1],[2]])\n",
    "y_train = torch.FloatTensor([[3],[5]])\n",
    "# 내가 원하는 함수 y=wx+b-> y= 2x + 1 , 단 b = 0 으로 초기값을 줌  \n",
    "# 모델 초기화\n",
    "W = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr=0.1)\n",
    "\n",
    "nb_epochs = 10 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산, \"선형\"회귀 이므로 parameter w,b에 대한 직선을 찾아내는거 목적이다.\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산 (임의의 직선 y와 실제 y의 오차 계산)\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    print(\"w is : \", W)\n",
    "    print(\"b is : \", b)\n",
    "    print(\"W.grad is : \", W.grad)\n",
    "    print(\"b.grad is : \", b.grad)\n",
    "    print(\"cost is : \", cost)\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    print(\"W.grad is zero: \", W.grad)\n",
    "    print(\"b.grad is zero: \", b.grad)\n",
    "    # cost.backward()가 처리된 후 (W,b)를 뽑아내고 optimizer.step()으로(w,b)를 갱신함.\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print(\"W.grad is backward: \", W.grad)\n",
    "    print(\"b.grad is backward: \", b.grad)    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 1 == 0:\n",
    "        print('Epoch {}/{} W: {:.3f}, W.grad: {} b: {} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(),W.grad.item(), b.item(), cost.item()\n",
    "        ))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lawboto')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd46718e6bb4835b306234a061b0dbde8f47813339e2874b919b1a11634ac550"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
