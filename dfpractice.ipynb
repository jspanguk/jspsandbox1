{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_json\n",
      "{'id': 1, 'username': '박준성', 'email': 'steepcastle@naver.com', 'address': {'street': '율곡로', 'suite': 'Apt. 556', 'city': '서울', 'zipcode': '92998-3874'}}\n",
      "\n",
      "\n",
      "dict_to_df\n",
      "         id username                  email     address\n",
      "city      1      박준성  steepcastle@naver.com          서울\n",
      "street    1      박준성  steepcastle@naver.com         율곡로\n",
      "suite     1      박준성  steepcastle@naver.com    Apt. 556\n",
      "zipcode   1      박준성  steepcastle@naver.com  92998-3874\n",
      "\n",
      "\n",
      "df_to_json\n",
      "{\n",
      "    \"id\":{\n",
      "        \"city\":1,\n",
      "        \"street\":1,\n",
      "        \"suite\":1,\n",
      "        \"zipcode\":1\n",
      "    },\n",
      "    \"username\":{\n",
      "        \"city\":\"박준성\",\n",
      "        \"street\":\"박준성\",\n",
      "        \"suite\":\"박준성\",\n",
      "        \"zipcode\":\"박준성\"\n",
      "    },\n",
      "    \"email\":{\n",
      "        \"city\":\"steepcastle@naver.com\",\n",
      "        \"street\":\"steepcastle@naver.com\",\n",
      "        \"suite\":\"steepcastle@naver.com\",\n",
      "        \"zipcode\":\"steepcastle@naver.com\"\n",
      "    },\n",
      "    \"address\":{\n",
      "        \"city\":\"서울\",\n",
      "        \"street\":\"율곡로\",\n",
      "        \"suite\":\"Apt. 556\",\n",
      "        \"zipcode\":\"92998-3874\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "df_to_dict\n",
      "{'id': {'city': 1, 'street': 1, 'suite': 1, 'zipcode': 1}, 'username': {'city': '박준성', 'street': '박준성', 'suite': '박준성', 'zipcode': '박준성'}, 'email': {'city': 'steepcastle@naver.com', 'street': 'steepcastle@naver.com', 'suite': 'steepcastle@naver.com', 'zipcode': 'steepcastle@naver.com'}, 'address': {'city': '서울', 'street': '율곡로', 'suite': 'Apt. 556', 'zipcode': '92998-3874'}}\n",
      "\n",
      "\n",
      "dict_json\n",
      "{'id': 1, 'username': '박준성', 'email': 'steepcastle@naver.com', 'address': {'street': '율곡로', 'suite': 'Apt. 556', 'city': '서울', 'zipcode': '92998-3874'}}\n",
      "\n",
      "\n",
      "dict_to_json\n",
      "{\"id\": 1, \"username\": \"박준성\", \"email\": \"steepcastle@naver.com\", \"address\": {\"street\": \"율곡로\", \"suite\": \"Apt. 556\", \"city\": \"서울\", \"zipcode\": \"92998-3874\"}}\n",
      "\n",
      "\n",
      "df_to_json_dumps\n",
      "b'\\x80\\x04\\x958\\x02\\x00\\x00\\x00\\x00\\x00\\x00X1\\x02\\x00\\x00{\\n    \"id\":{\\n        \"city\":1,\\n        \"street\":1,\\n        \"suite\":1,\\n        \"zipcode\":1\\n    },\\n    \"username\":{\\n        \"city\":\"\\xeb\\xb0\\x95\\xec\\xa4\\x80\\xec\\x84\\xb1\",\\n        \"street\":\"\\xeb\\xb0\\x95\\xec\\xa4\\x80\\xec\\x84\\xb1\",\\n        \"suite\":\"\\xeb\\xb0\\x95\\xec\\xa4\\x80\\xec\\x84\\xb1\",\\n        \"zipcode\":\"\\xeb\\xb0\\x95\\xec\\xa4\\x80\\xec\\x84\\xb1\"\\n    },\\n    \"email\":{\\n        \"city\":\"steepcastle@naver.com\",\\n        \"street\":\"steepcastle@naver.com\",\\n        \"suite\":\"steepcastle@naver.com\",\\n        \"zipcode\":\"steepcastle@naver.com\"\\n    },\\n    \"address\":{\\n        \"city\":\"\\xec\\x84\\x9c\\xec\\x9a\\xb8\",\\n        \"street\":\"\\xec\\x9c\\xa8\\xea\\xb3\\xa1\\xeb\\xa1\\x9c\",\\n        \"suite\":\"Apt. 556\",\\n        \"zipcode\":\"92998-3874\"\\n    }\\n}\\x94.'\n",
      "\n",
      "\n",
      "type(df_to_json_dumps)\n",
      "<class 'bytes'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bytes_to_obj\n",
      "{\n",
      "    \"id\":{\n",
      "        \"city\":1,\n",
      "        \"street\":1,\n",
      "        \"suite\":1,\n",
      "        \"zipcode\":1\n",
      "    },\n",
      "    \"username\":{\n",
      "        \"city\":\"박준성\",\n",
      "        \"street\":\"박준성\",\n",
      "        \"suite\":\"박준성\",\n",
      "        \"zipcode\":\"박준성\"\n",
      "    },\n",
      "    \"email\":{\n",
      "        \"city\":\"steepcastle@naver.com\",\n",
      "        \"street\":\"steepcastle@naver.com\",\n",
      "        \"suite\":\"steepcastle@naver.com\",\n",
      "        \"zipcode\":\"steepcastle@naver.com\"\n",
      "    },\n",
      "    \"address\":{\n",
      "        \"city\":\"서울\",\n",
      "        \"street\":\"율곡로\",\n",
      "        \"suite\":\"Apt. 556\",\n",
      "        \"zipcode\":\"92998-3874\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "type(bytes_to_obj)\n",
      "<class 'str'>\n",
      "\n",
      "\n",
      "df_ex\n",
      "   new_add  id username                  email     address\n",
      "0     city   1      박준성  steepcastle@naver.com          서울\n",
      "1   street   1      박준성  steepcastle@naver.com         율곡로\n",
      "2    suite   1      박준성  steepcastle@naver.com    Apt. 556\n",
      "3  zipcode   1      박준성  steepcastle@naver.com  92998-3874\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "df_to_nested_json\n",
      "[{'id': 1, 'username': '박준성', 'email': 'steepcastle@naver.com', 'address': {'city': '서울', 'street': '율곡로', 'suite': 'Apt. 556', 'zipcode': '92998-3874'}}]\n",
      "nested_json\n",
      "[{\"id\": 1, \"username\": \"\\ubc15\\uc900\\uc131\", \"email\": \"steepcastle@naver.com\", \"address\": {\"city\": \"\\uc11c\\uc6b8\", \"street\": \"\\uc728\\uace1\\ub85c\", \"suite\": \"Apt. 556\", \"zipcode\": \"92998-3874\"}}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dict_json = {                       \n",
    "    \"id\": 1,\n",
    "    \"username\": \"박준성\",\n",
    "    \"email\": \"steepcastle@naver.com\",\n",
    "    \"address\": {\n",
    "        \"street\": \"율곡로\",\n",
    "        \"suite\": \"Apt. 556\",\n",
    "        \"city\": \"서울\",\n",
    "        \"zipcode\": \"92998-3874\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"dict_json\")\n",
    "print(dict_json)\n",
    "print(\"\\n\")\n",
    "\n",
    "dict_to_df = pd.DataFrame(dict_json)                 #dict를 dataframe 으로 변환\n",
    "print(\"dict_to_df\")\n",
    "print(dict_to_df)\n",
    "print(\"\\n\")\n",
    "\n",
    "df_to_json = dict_to_df.to_json(force_ascii=False, indent = 4)         #dataframe 을 json으로 변환해주는 매서드 to_json이다. 열이름:{행이름:시리즈값} 을 전부 출력한다.\n",
    "print(\"df_to_json\")\n",
    "print(df_to_json)                                    #json 변환시 한글깨짐 현상이 일어난다. 아스키코드에는 한글이 없기때문이다. 아스키코드를 비활성화하여 깨짐을 막는다.\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "df_to_dict = dict_to_df.to_dict()                       #df 를 dict로 변환했다. 출력값을 보면 \"가 '로 바뀌고 띄어쓰기의 차이만 있을 뿐 dict와 json string은 사실상 다를게 없는 것처럼 보인다.\n",
    "print(\"df_to_dict\")\n",
    "print(df_to_dict)                                       #하지만 json string은 _str로 dict는 dict(_str,any) 로 파이썬 객체가 다르다.\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"dict_json\")\n",
    "print(dict_json)                                        #dict_json 과 dict_to_json은 출력값이 따옴표만 다르고 완전히 같다. dict(파이썬 자료형 객체)냐 str(문자열)의 차이만 있는셈\n",
    "print(\"\\n\")\n",
    "\n",
    "dict_to_json = json.dumps(dict_json,ensure_ascii=False)    #json.dumps는 python의 obj를 json str으로 변환한다. json.dump와의 차이점은 dump는 fp(=file point)를 지정하여\n",
    "print(\"dict_to_json\")\n",
    "print(dict_to_json)                                                  #메모리에서 저장장치로 json str을 내보내지만 dumps는 객체를 json str로 변환만한다.(=메모리에만 떠있다)\n",
    "print(\"\\n\")\n",
    "\n",
    "# with open('dfjs.json', 'w') as f:   #Object of type DataFrame is not JSON serializable 이라는 에러가 나온다. 간단히 dataframe은 json.dump로는 json str로 변환할수 없다는 것이다.(to_json매서드 사용필요)\n",
    "#     json.dump(dfjs, f, indent=2)     #\n",
    "\n",
    "\n",
    "# with open('df_to_json.pickle', 'w') as f:              # dataframe을 json string 으로 변환한 객체를 pickle 모듈을 이용해 기억장치로 저장하는 함수다.\n",
    "#     pickle.dump(df_to_json, f, pickle.HIGHEST_PROTOCOL)   #pickle.dump 매서드로 json string으로 변환된 파이썬 객체를 기억장치로 내보낸다.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dict_to_df = dict_to_df.reset_index(level=0)\n",
    "# print('resetindex')\n",
    "# print(dict_to_df)\n",
    "# print('\\n')\n",
    "\n",
    "# dict_to_df = dict_to_df.groupby('index')['address'].apply().to_dict()\n",
    "\n",
    "\n",
    "df_to_json_dumps = pickle.dumps(df_to_json)   # dataframe을 json string으로 만든 객체를 pickle.dumps매서드를 이용하여 bytes구조로 메모리에 띄운다.\n",
    "                                                # 메모리에 객체를 생성하는 것이지 파일을 만들어 기억장치로 보내는게 아니다.\n",
    "\n",
    "# with open(df_to_json_dumps,\"r\") as f:         # UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte 가 출력된다.\n",
    "#     data = pickle.loads(f)\n",
    "# print(data)\n",
    "\n",
    "print(\"df_to_json_dumps\")\n",
    "print(df_to_json_dumps)                                 #dumps매서드를 사용하여 bytes 로 변환되었기에 b' 으로 시작하는 문자열이 만들어진다.\n",
    "print(\"\\n\")\n",
    "print(\"type(df_to_json_dumps)\")\n",
    "print(type(df_to_json_dumps))                           # bytes는 원시 이진데이터나 1바이트 문자(영어,숫자)로 표현하기 위하여 사용된다. 그런데 한글은 최소2바이트에서 3바이트로 처리하므로 언어가 깨진다.\n",
    "print(\"\\n\")\n",
    "bytes_to_obj = pickle.loads(df_to_json_dumps)           # df_to_json_dumps는 bytes로 변환된 객체다. 이녀석을 그대로 print하면 위에서 설명한 것처럼 바이트크기가 달라 한글이 깨진다.\n",
    "print(\"\\n\")\n",
    "print(\"bytes_to_obj\")\n",
    "print(bytes_to_obj)                                     # 그러므로 pickle.loads(bytes) 매서드를 사용하여 bytes를 파이썬객체 obj로 변환 후 프린트하여야 한다.\n",
    "print(\"\\n\")\n",
    "print(\"type(bytes_to_obj)\")\n",
    "print(type(bytes_to_obj))\n",
    "print(\"\\n\")\n",
    "\n",
    "df_ex = dict_to_df.reset_index().rename(columns={'index':'new_add'})\n",
    "\n",
    "def mk_zip_to_dict(sr) :\n",
    "    gdf = zip(sr.new_add, sr.address)\n",
    "    d = {}\n",
    "    for k,v in gdf :\n",
    "        d[k] = v\n",
    "    \n",
    "    return d\n",
    "print('df_ex')\n",
    "print(df_ex)\n",
    "print('\\n')\n",
    "df_to_nested_json = (df_ex.groupby(['id', 'username', 'email'])\n",
    "              .apply(lambda x : mk_zip_to_dict(x))\n",
    "              .reset_index()\n",
    "             .rename(columns={0:\"address\"})\n",
    "             .to_dict('records'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gorupdf\n",
      "             new_add     pred id ts value       features\n",
      "0                col  0.50726  b  b     A           aaaa\n",
      "1        create_date  0.50726  b  b     A  1582089665000\n",
      "2  first_create_date  0.50726  b  b     A  1582089665000\n",
      "3            revenue  0.50726  b  b     A             nu\n",
      "4   update_timestamp  0.50726  b  b     A  1582142462000\n",
      "5                  x  0.50726  b  b     A             nu\n",
      "\n",
      "\n",
      "groupdf_to_list_of_dict\n",
      "[{'id': 'b', 'pred': 0.50726, 'ts': 'b', 'value': 'A', 'nested_group': {'col': 'aaaa', 'create_date': 1582089665000, 'first_create_date': 1582089665000, 'revenue': 'nu', 'update_timestamp': 1582142462000, 'x': 'nu'}}]\n",
      "{\n",
      "    \"id\": \"b\",\n",
      "    \"pred\": 0.50726,\n",
      "    \"ts\": \"b\",\n",
      "    \"value\": \"A\",\n",
      "    \"nested_group\": {\n",
      "        \"col\": \"aaaa\",\n",
      "        \"create_date\": 1582089665000,\n",
      "        \"first_create_date\": 1582089665000,\n",
      "        \"revenue\": \"nu\",\n",
      "        \"update_timestamp\": 1582142462000,\n",
      "        \"x\": \"nu\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "dict_sample ={\n",
    "\n",
    "    \"pred\": 0.50726,\n",
    "    \"id\": \"b\",\n",
    "    \"ts\": \"b\",\n",
    "    \"value\": \"A\",\n",
    "    \"features\": \n",
    "\n",
    "        {\n",
    "\n",
    "        \"first_create_date\": 1582089665000,\n",
    "        \"create_date\": 1582089665000,\n",
    "        \"update_timestamp\": 1582142462000,\n",
    "        \"revenue\": \"nu\",\n",
    "        \"col\":\"aaaa\",\n",
    "        \"x\": \"nu\"\n",
    "        \n",
    "        }\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dict_sample)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "groupdf = df.reset_index().rename(columns={'index':'new_add'})\n",
    "\n",
    "# df의 index을 reset_inde() 메서드로 column으로 변환, rename()메서드로 column명을 바꿈.\n",
    "# dataframe index는 reset_index 후 컬럼명 index로 하나의 컬럼으로 만들어지고 시리즈의 경우에는 index가 아니라 \"0\"이 컬럼명이 되어 시리즈가 dataframe으로 변환됨\n",
    "\n",
    "print('gorupdf')\n",
    "print(groupdf)\n",
    "print('\\n')\n",
    "\n",
    "def mk_zip_to_dict(df) :\n",
    "\n",
    "    ziped_dict = zip(df.new_add, df.features)\n",
    "    d = {}\n",
    "    for k,v in ziped_dict :\n",
    "        d[k] = v\n",
    "    \n",
    "    return d\n",
    "\n",
    "# 사용자지정함수를 생성. 인자는 dataframe 객체가 될 것이다. 파이썬 내부함수 zip()을 사용하여 df의 columns('new_add', 'features')를 d라는 딕셔너리의 key 와 value로 넣었다.\n",
    "\n",
    "groupdf_to_list_of_dict = (groupdf.groupby(['id','pred','ts','value'])\n",
    "            .apply(lambda x: mk_zip_to_dict(x))\n",
    "            .reset_index()\n",
    "            .rename(columns={0:'nested_group'})\n",
    "            .to_dict(orient='records'))\n",
    "\n",
    "# groupby 메서드로 groupdf의 columns ['id','pred','ts','value']를 key로 하는 dataframegroupby 객체를 생성\n",
    "# 이후 apply 메서드로 위에서 만든 사용자 지정함수를 적용하였다. 인자 x 는 groupdf이다.\n",
    "# 이 과정에서 반환되는 객체는 series 이다. groupby로 멀티인덱스가 만들어졌지만 값이 apply메서드로 만들어진 dict 뿐이기 때문이다.\n",
    "# reset_index()를 사용하여 index를 column으로 변환하였다. apply로 만들어진 dict열에 reset_index로 변환된 column이 열이름으로 설정되었다.\n",
    "# reset_index()가 series를 dataframe 객체로 반환되었다.\n",
    "# rename()메서드로 column 명을 바꾸고 to_dict(orient='records')메서드로 dataframe을 dict객체로 변환할 수 있다. \n",
    "# orient로 'records'를 사용할 경우 반환되는 객체를 dict가 아니라 list의 형태로 반환된다는 것에 주의해야 한다.\n",
    "\n",
    "print('groupdf_to_list_of_dict')\n",
    "print(groupdf_to_list_of_dict)\n",
    "list_of_dict_to_dict = groupdf_to_list_of_dict[0]\n",
    "\n",
    "dict_to_json = json.dumps(list_of_dict_to_dict, indent=4)\n",
    "print(dict_to_json)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "#추가로 공부해봄 직한 녀석들\n",
    "\n",
    "# groupdf2 = (groupdf.groupby(['id','pred','ts','value'], as_index=False)\n",
    "#         .apply(lambda x: x[['new_index', 'features']].to_dict('records'))\n",
    "#        .reset_index()\n",
    "#        .rename(columns={0:'Tide-Data'})\n",
    "#        .to_json(orient='records'))\n",
    "        \n",
    "# groupdf_to_json = (groupdf.groupby(['id','pred','ts','value'], as_index=False)\n",
    "#             .apply(lambda x: x[['new_add', 'features']].to_dict('records')[0])\n",
    "#             .reset_index()\n",
    "#             .rename(columns={0:'nested_group'})\n",
    "#             .to_json(orient='records'))\n",
    "\n",
    "\n",
    "            \n",
    "# groupdf3 = groupdf2.to_dict('r')\n",
    "# print('groupdf_to_json')\n",
    "# print(groupdf_to_json)\n",
    "\n",
    "# print(groupdf3)\n",
    "# gr3 = mk_zip_to_nested_dict(groupdf)\n",
    "# print('gr3')\n",
    "# print(gr3)\n",
    "\n",
    "# nested_json = (df.groupby(['pred','id','ts','value'], as_index=False)\n",
    "#              .apply(lambda x:x[[\n",
    "#                 \"first_create_date\",  \n",
    "#                 \"create_date\",\n",
    "#                 \"update_timestamp\",\n",
    "#                 \"revenue\",\n",
    "#                 \"col\",\n",
    "#                 \"x\"]].to_dict('r'))\n",
    "#              .reset_index()\n",
    "#              .rename(columns={0:'features'})\n",
    "#              .to_json(orient='records'))\n",
    "            \n",
    "# df.groupby(['pred','id','ts','value'], as_index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_sample:\n",
      "\n",
      "{'features': {'col': 'aaaa',\n",
      "              'create_date': 1582089665000,\n",
      "              'first_create_date': 1582089665000,\n",
      "              'revenue': 'nu',\n",
      "              'update_timestamp': 1582142462000,\n",
      "              'x': 'nu'},\n",
      " 'id': 'b',\n",
      " 'pred': 0.50726,\n",
      " 'ts': 'b',\n",
      " 'value': 'A'}\n",
      "\n",
      "dict_sample_to_df:\n",
      "\n",
      "                      pred id ts value       features\n",
      "col                0.50726  b  b     A           aaaa\n",
      "create_date        0.50726  b  b     A  1582089665000\n",
      "first_create_date  0.50726  b  b     A  1582089665000\n",
      "revenue            0.50726  b  b     A             nu\n",
      "update_timestamp   0.50726  b  b     A  1582142462000\n",
      "x                  0.50726  b  b     A             nu\n",
      "\n",
      "gorupdf\n",
      "             new_add     pred id ts value       features\n",
      "0                col  0.50726  b  b     A           aaaa\n",
      "1        create_date  0.50726  b  b     A  1582089665000\n",
      "2  first_create_date  0.50726  b  b     A  1582089665000\n",
      "3            revenue  0.50726  b  b     A             nu\n",
      "4   update_timestamp  0.50726  b  b     A  1582142462000\n",
      "5                  x  0.50726  b  b     A             nu\n",
      "\n",
      "\n",
      "dff\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zkshq\\Documents\\VSCodeProjects\\jspsandbox1\\dfpractice.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m d\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m dff \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mfrom_dict(mk_zip_to_dict(groupdf, \u001b[39m\"\u001b[39;49m\u001b[39mnew_add\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfeatures\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mreset_index()\u001b[39m.\u001b[39mto_dict(orient \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(dff)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:1677\u001b[0m, in \u001b[0;36mDataFrame.from_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1674\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39monly recognize index or columns for orient\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1676\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1677\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(data, index\u001b[39m=\u001b[39;49mindex, columns\u001b[39m=\u001b[39;49mcolumns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1678\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1679\u001b[0m     realdata \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    637\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[39m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    663\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 664\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    666\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[0;32m    667\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "dict_sample ={\n",
    "\n",
    "    \"pred\": 0.50726,\n",
    "    \"id\": \"b\",\n",
    "    \"ts\": \"b\",\n",
    "    \"value\": \"A\",\n",
    "    \"features\": \n",
    "\n",
    "        {\n",
    "\n",
    "        \"first_create_date\": 1582089665000,\n",
    "        \"create_date\": 1582089665000,\n",
    "        \"update_timestamp\": 1582142462000,\n",
    "        \"revenue\": \"nu\",\n",
    "        \"col\":\"aaaa\",\n",
    "        \"x\": \"nu\"\n",
    "        \n",
    "        }\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dict_sample)\n",
    "print(\"dict_sample:\\n\")\n",
    "pprint.pprint(dict_sample)\n",
    "print(\"\")\n",
    "print(\"dict_sample_to_df:\\n\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "groupdf = df.reset_index().rename(columns={'index':'new_add'})\n",
    "\n",
    "# df의 index을 reset_inde() 메서드로 column으로 변환, rename()메서드로 column명을 바꿈.\n",
    "# dataframe index는 reset_index 후 컬럼명 index로 하나의 컬럼으로 만들어지고 시리즈의 경우에는 index가 아니라 \"0\"이 컬럼명이 되어 시리즈가 dataframe으로 변환됨\n",
    "\n",
    "print('gorupdf')\n",
    "print(groupdf)\n",
    "print('\\n')\n",
    "\n",
    "def mk_zip_to_dict(df, new_keys, new_values) :\n",
    "\n",
    "    gdf = zip(df[new_keys], df[new_values])\n",
    "    # zip 함수의 반환값 유형은 zip 타입이고 그 원소로 튜플을 가짐\n",
    "    d = {}\n",
    "    for k,v in gdf :\n",
    "        d[k] = v\n",
    "    \n",
    "    return d\n",
    "\n",
    "print(\"dff\")\n",
    "dff = pd.DataFrame.from_dict(mk_zip_to_dict(groupdf, \"new_add\", \"features\")).reset_index().to_dict(orient = 'records')\n",
    "print(dff)\n",
    "print()\n",
    "\n",
    "# 사용자지정함수를 생성. 인자는 dataframe 객체가 될 것이다. 파이썬 내부함수 zip()을 사용하여 df의 columns('new_add', 'features')를 d라는 딕셔너리의 key 와 value로 넣었다.\n",
    "groupedby = groupdf.groupby(['id','pred','ts','value'])\n",
    "\n",
    "print(\"groupedby:\")\n",
    "ii = 0\n",
    "for i in groupedby:\n",
    "    ii = ii + 1\n",
    "    print(\"count\" + str(ii))\n",
    "    print(i)\n",
    "    print()\n",
    "\n",
    "groupdf_to_list_of_dict = \\\n",
    "    groupedby \\\n",
    "    .apply(lambda x: mk_zip_to_dict(x, \"new_add\", \"features\")) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={0:'nested_group'}) \\\n",
    "    .to_dict(orient='records')\n",
    "\n",
    "\n",
    "# groupby 메서드로 groupdf의 columns ['id','pred','ts','value']를 key로 하는 dataframegroupby 객체를 생성\n",
    "# 이후 apply 메서드로 위에서 만든 사용자 지정함수를 적용하였다. 인자 x 는 groupdf이다.\n",
    "# 이 과정에서 반환되는 객체는 series 이다. groupby로 멀티인덱스가 만들어졌지만 값이 apply메서드로 만들어진 dict 뿐이기 때문이다.\n",
    "# reset_index()를 사용하여 index를 column으로 변환하였다. apply로 만들어진 dict열에 reset_index로 변환된 column이 열이름으로 설정되었다.\n",
    "# reset_index()가 series를 dataframe 객체로 반환되었다.\n",
    "# rename()메서드로 column 명을 바꾸고 to_dict(orient='records')메서드로 dataframe을 dict객체로 변환할 수 있다. \n",
    "# orient로 'records'를 사용할 경우 반환되는 객체를 dict가 아니라 list의 형태로 반환된다는 것에 주의해야 한다.\n",
    "\n",
    "print('groupdf_to_list_of_dict')\n",
    "print(groupdf_to_list_of_dict)\n",
    "\n",
    "\n",
    "print()\n",
    "list_of_dict_to_dict = groupdf_to_list_of_dict[0]\n",
    "\n",
    "dict_to_json = json.dumps(list_of_dict_to_dict, indent=4)\n",
    "print(dict_to_json)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "#추가로 공부해봄 직한 녀석들\n",
    "\n",
    "# groupdf2 = (groupdf.groupby(['id','pred','ts','value'], as_index=False)\n",
    "#         .apply(lambda x: x[['new_index', 'features']].to_dict('records'))\n",
    "#        .reset_index()\n",
    "#        .rename(columns={0:'Tide-Data'})\n",
    "#        .to_json(orient='records'))\n",
    "        \n",
    "# groupdf_to_json = (groupdf.groupby(['id','pred','ts','value'], as_index=False)\n",
    "#             .apply(lambda x: x[['new_add', 'features']].to_dict('records')[0])\n",
    "#             .reset_index()\n",
    "#             .rename(columns={0:'nested_group'})\n",
    "#             .to_json(orient='records'))\n",
    "\n",
    "\n",
    "            \n",
    "# groupdf3 = groupdf2.to_dict('r')\n",
    "# print('groupdf_to_json')\n",
    "# print(groupdf_to_json)\n",
    "\n",
    "# print(groupdf3)\n",
    "# gr3 = mk_zip_to_nested_dict(groupdf)\n",
    "# print('gr3')\n",
    "# print(gr3)\n",
    "\n",
    "# nested_json = (df.groupby(['pred','id','ts','value'], as_index=False)\n",
    "#              .apply(lambda x:x[[\n",
    "#                 \"first_create_date\",  \n",
    "#                 \"create_date\",\n",
    "#                 \"update_timestamp\",\n",
    "#                 \"revenue\",\n",
    "#                 \"col\",\n",
    "#                 \"x\"]].to_dict('r'))\n",
    "#              .reset_index()\n",
    "#              .rename(columns={0:'features'})\n",
    "#              .to_json(orient='records'))\n",
    "            \n",
    "# df.groupby(['pred','id','ts','value'], as_index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['steepcastle1', 'steepcastle2', 'steepcastle3', 'steepcastle4'], 'username': ['박준성', '박준택', '밤밤이', '껄룩이'], 'email': ['steepcastle@naver.com', 'juntaeks', 'bambami', 'takealook'], 'address': ['서울', '은평구', '가좌로', '7라길']}\n",
      "dict_to_df\n",
      "             id username                  email address\n",
      "0  steepcastle1      박준성  steepcastle@naver.com      서울\n",
      "1  steepcastle2      박준택               juntaeks     은평구\n",
      "2  steepcastle3      밤밤이                bambami     가좌로\n",
      "3  steepcastle4      껄룩이              takealook     7라길\n",
      "\n",
      "\n",
      "df_to_json\n",
      "{\"id\":{\"0\":\"steepcastle1\",\"1\":\"steepcastle2\",\"2\":\"steepcastle3\",\"3\":\"steepcastle4\"},\"username\":{\"0\":\"박준성\",\"1\":\"박준택\",\"2\":\"밤밤이\",\"3\":\"껄룩이\"},\"email\":{\"0\":\"steepcastle@naver.com\",\"1\":\"juntaeks\",\"2\":\"bambami\",\"3\":\"takealook\"},\"address\":{\"0\":\"서울\",\"1\":\"은평구\",\"2\":\"가좌로\",\"3\":\"7라길\"}}\n",
      "\n",
      "\n",
      "df_to_dict\n",
      "{'id': ['steepcastle1', 'steepcastle2', 'steepcastle3', 'steepcastle4'], 'username': ['박준성', '박준택', '밤밤이', '껄룩이'], 'email': ['steepcastle@naver.com', 'juntaeks', 'bambami', 'takealook'], 'address': ['서울', '은평구', '가좌로', '7라길']}\n",
      "\n",
      "\n",
      "dict_json\n",
      "{'id': ['steepcastle1', 'steepcastle2', 'steepcastle3', 'steepcastle4'], 'username': ['박준성', '박준택', '밤밤이', '껄룩이'], 'email': ['steepcastle@naver.com', 'juntaeks', 'bambami', 'takealook'], 'address': ['서울', '은평구', '가좌로', '7라길']}\n",
      "\n",
      "\n",
      "dict_to_json\n",
      "{\"id\": [\"steepcastle1\", \"steepcastle2\", \"steepcastle3\", \"steepcastle4\"], \"username\": [\"박준성\", \"박준택\", \"밤밤이\", \"껄룩이\"], \"email\": [\"steepcastle@naver.com\", \"juntaeks\", \"bambami\", \"takealook\"], \"address\": [\"서울\", \"은평구\", \"가좌로\", \"7라길\"]}\n",
      "\n",
      "\n",
      "df_to_json_dumps b'\\x80\\x04\\x95B\\x01\\x00\\x00\\x00\\x00\\x00\\x00X;\\x01\\x00\\x00{\"id\":{\"0\":\"steepcastle1\",\"1\":\"steepcastle2\",\"2\":\"steepcastle3\",\"3\":\"steepcastle4\"},\"username\":{\"0\":\"\\xeb\\xb0\\x95\\xec\\xa4\\x80\\xec\\x84\\xb1\",\"1\":\"\\xeb\\xb0\\x95\\xec\\xa4\\x80\\xed\\x83\\x9d\",\"2\":\"\\xeb\\xb0\\xa4\\xeb\\xb0\\xa4\\xec\\x9d\\xb4\",\"3\":\"\\xea\\xbb\\x84\\xeb\\xa3\\xa9\\xec\\x9d\\xb4\"},\"email\":{\"0\":\"steepcastle@naver.com\",\"1\":\"juntaeks\",\"2\":\"bambami\",\"3\":\"takealook\"},\"address\":{\"0\":\"\\xec\\x84\\x9c\\xec\\x9a\\xb8\",\"1\":\"\\xec\\x9d\\x80\\xed\\x8f\\x89\\xea\\xb5\\xac\",\"2\":\"\\xea\\xb0\\x80\\xec\\xa2\\x8c\\xeb\\xa1\\x9c\",\"3\":\"7\\xeb\\x9d\\xbc\\xea\\xb8\\xb8\"}}\\x94.'\n",
      "<class 'bytes'>\n",
      "bytes_to_obj {\"id\":{\"0\":\"steepcastle1\",\"1\":\"steepcastle2\",\"2\":\"steepcastle3\",\"3\":\"steepcastle4\"},\"username\":{\"0\":\"박준성\",\"1\":\"박준택\",\"2\":\"밤밤이\",\"3\":\"껄룩이\"},\"email\":{\"0\":\"steepcastle@naver.com\",\"1\":\"juntaeks\",\"2\":\"bambami\",\"3\":\"takealook\"},\"address\":{\"0\":\"서울\",\"1\":\"은평구\",\"2\":\"가좌로\",\"3\":\"7라길\"}}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_json2 = {\n",
    "    \"id\" : [\"steepcastle1\",\"steepcastle2\",\"steepcastle3\",\"steepcastle4\"],\n",
    "    \"username\" : [\"박준성\",\"박준택\",\"밤밤이\",\"껄룩이\"],\n",
    "    \"email\" : [\"steepcastle@naver.com\",\"juntaeks\",\"bambami\",\"takealook\"],\n",
    "    \"address\" : [\"서울\",\"은평구\",\"가좌로\",\"7라길\"]\n",
    "    \n",
    "}\n",
    "\n",
    "print(dict_json2)\n",
    "\n",
    "\n",
    "dict_to_df = pd.DataFrame(dict_json2)                 #dict를 dataframe 으로 변환\n",
    "print(\"dict_to_df\")\n",
    "print(dict_to_df)\n",
    "print(\"\\n\")\n",
    "\n",
    "df_to_json = dict_to_df.to_json(force_ascii=False)         #dataframe 을 json으로 변환해주는 매서드 to_json이다. 열이름:{행이름:시리즈값} 을 전부 출력한다.\n",
    "print(\"df_to_json\")\n",
    "print(df_to_json)                                    #json 변환시 한글깨짐 현상이 일어난다. 아스키코드에는 한글이 없기때문이다. 아스키코드를 비활성화하여 깨짐을 막는다.\n",
    "print(\"\\n\")\n",
    "\n",
    "df_to_dict = dict_to_df.to_dict('list')                       #df 를 dict로 변환했다. 출력값을 보면 \"가 '로 바뀌고 띄어쓰기의 차이만 있을 뿐 dict와 json string은 사실상 다를게 없는 것처럼 보인다.\n",
    "print(\"df_to_dict\")\n",
    "print(df_to_dict)                                       #하지만 json string은 _str로 dict는 dict(_str,any) 로 파이썬 객체가 다르다.\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"dict_json\")\n",
    "print(dict_json2)                                        #dict_json 과 dict_to_json은 출력값이 따옴표만 다르고 완전히 같다. dict(파이썬 자료형 객체)냐 str(문자열)의 차이만 있는셈\n",
    "print(\"\\n\")\n",
    "\n",
    "dict_to_json = json.dumps(dict_json2,ensure_ascii=False)    #json.dumps는 python의 obj를 json str으로 변환한다. json.dump와의 차이점은 dump는 fp(=file point)를 지정하여\n",
    "print(\"dict_to_json\") \n",
    "print(dict_to_json)                                                  #메모리에서 저장장치로 json str을 내보내지만 dumps는 객체를 json str로 변환만한다.(=메모리에만 떠있다)\n",
    "print(\"\\n\")\n",
    "\n",
    "# with open('dfjs.json', 'w') as f:   #Object of type DataFrame is not JSON serializable 이라는 에러가 나온다. 간단히 dataframe은 json.dump로는 json str로 변환할수 없다는 것이다.(to_json매서드 사용필요)\n",
    "#     json.dump(dfjs, f, indent=2)     #\n",
    "\n",
    "\n",
    "# with open('df_to_json.pickle', 'w') as f:              # dataframe을 json string 으로 변환한 객체를 pickle 모듈을 이용해 기억장치로 저장하는 함수다.\n",
    "#     pickle.dump(df_to_json, f, pickle.HIGHEST_PROTOCOL)   #pickle.dump 매서드로 json string으로 변환된 파이썬 객체를 기억장치로 내보낸다.\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "df_to_json_dumps = pickle.dumps(df_to_json)   # dataframe을 json string으로 만든 객체를 pickle.dumps매서드를 이용하여 bytes구조로 메모리에 띄운다.\n",
    "                                                # 메모리에 객체를 생성하는 것이지 파일을 만들어 기억장치로 보내는게 아니다.\n",
    "\n",
    "# with open(df_to_json_dumps,\"r\") as f:         # UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte 가 출력된다.\n",
    "#     data = pickle.loads(f)\n",
    "# print(data)\n",
    "\n",
    "\n",
    "print(\"df_to_json_dumps\",df_to_json_dumps)                                 #dumps매서드를 사용하여 bytes 로 변환되었기에 b' 으로 시작하는 문자열이 만들어진다.\n",
    "print(type(df_to_json_dumps))                           # bytes는 원시 이진데이터나 1바이트 문자(영어,숫자)로 표현하기 위하여 사용된다. 그런데 한글은 최소2바이트에서 3바이트로 처리하므로 언어가 깨진다.\n",
    "bytes_to_obj = pickle.loads(df_to_json_dumps)           # df_to_json_dumps는 bytes로 변환된 객체다. 이녀석을 그대로 print하면 위에서 설명한 것처럼 바이트크기가 달라 한글이 깨진다.\n",
    "print(\"bytes_to_obj\", bytes_to_obj)                                     # 그러므로 pickle.loads(bytes) 매서드를 사용하여 bytes를 파이썬객체 obj로 변환 후 프린트하여야 한다.\n",
    "print(type(bytes_to_obj))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zkshq\\Documents\\VSCodeProjects\\jspsandbox1\n",
      "df_summary_fullest.csv\n",
      "c:\\Users\\zkshq\\Documents\\VSCodeProjects\\jspsandbox1\\df_summary_fullest.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zkshq\\Documents\\VSCodeProjects\\jspsandbox1\\dfpractice.ipynb 셀 5\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(b, f, ensure_ascii \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#인풋내용\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# 1) dftojson.json (dump)        2) jsontostring.json  (dumps)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m df_to_json_dump(dfhead)       \u001b[39m#df를 dict로 변환후 json 으로 직렬화 이후 내보냄\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m json_to_jsondumps \u001b[39m=\u001b[39m df_to_json_dump(dfhead)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m json\u001b[39m.\u001b[39mdumps(json_to_jsondumps)         \u001b[39m#json string 저장 파일명 입력, string 문자열은 한줄로 표현되기에 가시성이 안좋으므로 들여쓰기(=>indent=n [n은정수]) 추가하는게 좋다\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\zkshq\\Documents\\VSCodeProjects\\jspsandbox1\\dfpractice.ipynb 셀 5\u001b[0m in \u001b[0;36mdf_to_json_dump\u001b[1;34m(dfhead)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m mkfile_name \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m파일명.json을 입력하시오.(ex : bald.json) : \u001b[39m\u001b[39m\"\u001b[39m)   \u001b[39m#text 그대로가 아닌 json 바이너리로 내보내므로 한글이 깨지는 현상이 발생한다. 이는 json.dump 매서드가 내부의 자료형을 ascii 로 자동으로 변환하기 때문이며\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                                             \u001b[39m#이를 예방하고 싶다면 ensure_ascii = False 를 입력하며 아스키코드 인코딩을 False 로 해주면 된다.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(mkfile_name, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zkshq/Documents/VSCodeProjects/jspsandbox1/dfpractice.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   json\u001b[39m.\u001b[39mdump(b, f, ensure_ascii \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "from fileinput import close\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy\n",
    "import json\n",
    "\n",
    "path = os.getcwd() #실행폴더 확인\n",
    "print(path)\n",
    "file_name = 'df_summary_fullest.csv' #실행파일 확인\n",
    "print(file_name)\n",
    "file_path = os.path.join(path,file_name)  # directory, file 경로 합침 \n",
    "print(file_path)\n",
    "\n",
    " \n",
    "df = pd.read_csv(file_path, encoding='utf-8')        #불러올 파일의 경로, 폰트깨짐을 막기위해 파일별로 적정한 인코딩방식\n",
    "dfhead = df.head(2)    #json모듈을 사용하는 dump, dumps 는 dataframe 타입을 json으로 변환하지 못한다.\n",
    "\n",
    "\n",
    "\n",
    "def df_to_json_dump(dfhead) :                #json.dump의 경우 본래는 df구조를 json으로 변환하지 못한다. 따라서 df를 적절한 자료형으로 가공한후 다시 json으로 재가공 해야한다\n",
    "  b = dfhead.to_dict()                        #이 사용자지정함수는 인자로 받은 객체(=dataframe)을 to_dict 매서드로 dict객체로 변환하고 이를 사용자가 입력한 json파일로 다시 변환하여 내보낸다.\n",
    "  mkfile_name = input(\"파일명.json을 입력하시오.(ex : bald.json) : \")   #text 그대로가 아닌 json 바이너리로 내보내므로 한글이 깨지는 현상이 발생한다. 이는 json.dump 매서드가 내부의 자료형을 ascii 로 자동으로 변환하기 때문이며\n",
    "                                              #이를 예방하고 싶다면 ensure_ascii = False 를 입력하며 아스키코드 인코딩을 False 로 해주면 된다.\n",
    "  with open(mkfile_name, 'w') as f :\n",
    "    json.dump(b, f, ensure_ascii = False)\n",
    "\n",
    "#인풋내용\n",
    "# 1) dftojson.json (dump)        2) jsontostring.json  (dumps)\n",
    "\n",
    "df_to_json_dump(dfhead)       #df를 dict로 변환후 json 으로 직렬화 이후 내보냄\n",
    "\n",
    "\n",
    "json_to_jsondumps = df_to_json_dump(dfhead)\n",
    "json.dumps(json_to_jsondumps)         #json string 저장 파일명 입력, string 문자열은 한줄로 표현되기에 가시성이 안좋으므로 들여쓰기(=>indent=n [n은정수]) 추가하는게 좋다\n",
    " \n",
    "#json.dumps는 현재 메모리에 띄워져 있는 json 자료형을 json string 으로 변환해주는 매서드다. 따라서 json객체로 자료를 변환해주는 선행작업이 필수다.\n",
    "#TypeError: Object of type DataFrame is not JSON serializable 데이터프레임 객체는 json.dumps 매서드로는 json으로 직렬화가 불가능하다는 에러이다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"dftojson.json\", \"r\") as l:\n",
    "  l_load = json.load(l)\n",
    "  print(l_load)\n",
    "#내보낸 json 바이너리를 읽기모드로 불러와 메모리에 파이썬객체로 띄운다.(=역직렬화)\n",
    "\n",
    "json_loads = json.loads(json.dumps(json_to_jsondumps))\n",
    "print(json_loads)\n",
    "#json.loads 는 json string 을 파이썬객체로 역직렬화\n",
    "#json.load 는 파일에서 json 형식 데이터를 읽고 파이썬객체로 역직렬화\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "   a   b  c   d\n",
      "0  1  b1  2  d1\n",
      "1  1  b1  3  d2\n",
      "nested_json1\n",
      "[{'a': '1', 'b': 'b1', 'nested_group': {'2': 'd1', '3': 'd2'}}]\n",
      "{'a': '1', 'b': 'b1', 'nested_group': {'2': 'd1', '3': 'd2'}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "array = np.array([[1, 'b1', 2, 'd1'], [1, 'b1', 3, 'd2']])\n",
    "df = pd.DataFrame(data=array, columns=['a','b','c','d'])\n",
    "print('df')\n",
    "print(df)\n",
    "\n",
    "def mk_zip_to_dict(sr) :\n",
    "    gdf = zip(sr.c, sr.d)\n",
    "    d = {}\n",
    "    for k,v in gdf :\n",
    "        d[k] = v\n",
    "    \n",
    "    return d\n",
    "\n",
    "nested_json1 = (df.groupby(['a','b']) \n",
    "      .apply(lambda x: mk_zip_to_dict(x))\n",
    "      \n",
    "      .reset_index() \n",
    "      .rename(columns={0:'nested_group'})\n",
    "      .to_dict('records'))\n",
    "    #   .to_dict(orient = 'records'))\n",
    "\n",
    "# nested_json2 = (df.groupby(['a', 'b'])\n",
    "#         .apply(lambda x : x[['c', 'd']].to_dict('records'))\n",
    "#         .reset_index()\n",
    "#         .rename(columns={0:'nested_group'})\n",
    "#         .to_dict('records'))\n",
    "\n",
    "print('nested_json1')\n",
    "print(nested_json1)\n",
    "# print('nested_json2')\n",
    "# print(nested_json2)\n",
    "list_of_dict_to_dict = nested_json1[0]\n",
    "print(list_of_dict_to_dict)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
